{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goals: \n",
    "<ol>\n",
    "<li>Create a cleaned development dataset that can be used to complete the modeling step of this project\n",
    "    <ul>\n",
    "<li> Perform NLP Precrocessing steps to the text</li> \n",
    "<li>Split into testing and training datasets</li> \n",
    "<li>Vectorizing our dataset</li>\n",
    "    </ul>\n",
    "<li>Modeling: Build a <b>Negative Tweet Detector</b> </li>\n",
    "    <ul>\n",
    "<li>Building and evaluating models</li> \n",
    "<li>Comparing models</li> \n",
    "    </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4982, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>right now    we welcome competition  just no...</td>\n",
       "      <td>0.543</td>\n",
       "      <td>positive</td>\n",
       "      <td>83</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>hahaha  unfollowed     tile  a company who was...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>positive</td>\n",
       "      <td>104</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>i was thinking it might be in corenfc but i ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>this is super clever   creating a new battery ...</td>\n",
       "      <td>0.187</td>\n",
       "      <td>positive</td>\n",
       "      <td>98</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>any one be interested if i did an  airtag give...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>positive</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                      cleaned_tweet  polarity  \\\n",
       "0  2021-05-12    right now    we welcome competition  just no...     0.543   \n",
       "1  2021-05-12  hahaha  unfollowed     tile  a company who was...     0.100   \n",
       "2  2021-05-12    i was thinking it might be in corenfc but i ...     0.000   \n",
       "3  2021-05-12  this is super clever   creating a new battery ...     0.187   \n",
       "4  2021-05-12  any one be interested if i did an  airtag give...     0.250   \n",
       "\n",
       "  sentiment  text_len  text_word_count  \n",
       "0  positive        83               12  \n",
       "1  positive       104               17  \n",
       "2   neutral        94               17  \n",
       "3  positive        98               18  \n",
       "4  positive        52               10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cleaned_tweets.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in the last step, we cleaned the text of tweets after loading our dataset, and we've removed all the punctuations and lowercased the words. Now we need to perform some other preprocessing steps before fitting the data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in df['cleaned_tweet']]\n",
    "\n",
    "# Remove stop words\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "no_stops=[]\n",
    "for i in all_tokens:\n",
    "    new_no_stops = [t for t in i if t not in stopword]\n",
    "    no_stops.append(new_no_stops)\n",
    "\n",
    "# Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = []\n",
    "for i in no_stops:\n",
    "    for j in i:\n",
    "        new_lemmatized = [wordnet_lemmatizer.lemmatize(j) for j in i] #Lemmatize all tokens into a new list: lemmatized\n",
    "    lemmatized.append(new_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>right now    we welcome competition  just no...</td>\n",
       "      <td>0.543</td>\n",
       "      <td>positive</td>\n",
       "      <td>83</td>\n",
       "      <td>12</td>\n",
       "      <td>[right, welcome, competition, apple, tile, air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>hahaha  unfollowed     tile  a company who was...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>positive</td>\n",
       "      <td>104</td>\n",
       "      <td>17</td>\n",
       "      <td>[hahaha, unfollowed, tile, company, born, thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>i was thinking it might be in corenfc but i ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>[thinking, might, corenfc, seen, anything, spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>this is super clever   creating a new battery ...</td>\n",
       "      <td>0.187</td>\n",
       "      <td>positive</td>\n",
       "      <td>98</td>\n",
       "      <td>18</td>\n",
       "      <td>[super, clever, creating, new, battery, backpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>any one be interested if i did an  airtag give...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>positive</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>[one, interested, airtag, giveaway]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                      cleaned_tweet  polarity  \\\n",
       "0  2021-05-12    right now    we welcome competition  just no...     0.543   \n",
       "1  2021-05-12  hahaha  unfollowed     tile  a company who was...     0.100   \n",
       "2  2021-05-12    i was thinking it might be in corenfc but i ...     0.000   \n",
       "3  2021-05-12  this is super clever   creating a new battery ...     0.187   \n",
       "4  2021-05-12  any one be interested if i did an  airtag give...     0.250   \n",
       "\n",
       "  sentiment  text_len  text_word_count  \\\n",
       "0  positive        83               12   \n",
       "1  positive       104               17   \n",
       "2   neutral        94               17   \n",
       "3  positive        98               18   \n",
       "4  positive        52               10   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [right, welcome, competition, apple, tile, air...  \n",
       "1  [hahaha, unfollowed, tile, company, born, thri...  \n",
       "2  [thinking, might, corenfc, seen, anything, spe...  \n",
       "3  [super, clever, creating, new, battery, backpl...  \n",
       "4                [one, interested, airtag, giveaway]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed'] = lemmatized\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the feature and target that we will use for our model\n",
    "df = df[['preprocessed','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 CountVectorizer: Vectorizing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label positive and neutral sentiment as 0, and lable negative sentiment as 1\n",
    "df['negative_sentiment'] = df.sentiment.map({'positive':0,'neutral':0,'negative':1})\n",
    "y = df['negative_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preprocessed = df.preprocessed.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3337,)\n",
      "(1645,)\n",
      "(3337,)\n",
      "(1645,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['preprocessed'], y,test_size = 0.33,random_state = 33)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary and use it to create a document-term matrix: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix: count_test \n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000apple', '03', '05', '07', '07115164', '07airtag', '08132321259', '09', '0mm', '0ver', '10', '100', '10brain', '11', '119', '12', '120', '120m', '127', '128', '13', '132', '1379', '14', '149', '15', '1500', '15k', '16', '19', '1978', '1986', '19999', '1k', '1mi', '1st', '20', '2001', '2018', '2019', '2020', '2021', '2022', '21', '216', '237', '24', '25', '2516', '279', '280', '2837472', '29', '2fa', '2nd', '30', '300', '301', '30am', '30k', '319', '328', '32mb', '33', '33k', '349', '35', '35k', '360', '3d', '3dprinting', '3mm', '3rd', '3v', '40', '400', '400ft', '449', '479', '482', '486', '490', '499', '4agze', '4am', '4ever', '4k', '50', '500', '50m', '52', '52832', '53pm', '54', '5g', '5k', '5th', '5x', '60', '60m', '62', '658', '699', '6ft', '6user', '72', '75key', '7999', '7th', '835', '877', '8970', '8m', '8mm', '8th', '8v', '90', '900', '90deg', '95', '987', '99', '99link', '9to5m', '9to5mac', 'a2f6', 'a52', 'aapl', 'ab', 'abccentralvic', 'abcmsh', 'abcwimmera', 'abi', 'ability', 'able', 'absolute', 'absolutely', 'abt', 'abuse', 'abuser', 'abusive', 'ac', 'acc', 'acce', 'accelerometer', 'accept', 'acceptable', 'acces', 'access', 'accessibility', 'accessoriesairtags', 'accessory', 'accidentally', 'accommodation', 'accomplishment', 'accordingly', 'account', 'accuracy', 'accurate', 'accurately', 'acme', 'across', 'act', 'acting', 'activate', 'activated', 'activating', 'activation', 'active', 'actively', 'activity', 'actual', 'actually', 'ad', 'adafruit', 'adap', 'adaptor', 'add', 'added', 'adding', 'additional', 'addon', 'address', 'adequat', 'adequate', 'adjustable', 'advanced', 'advantage', 'advertised', 'advises', 'aesthetical', 'af', 'affair', 'affe', 'affiliate', 'affix', 'afford', 'affordability']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 200 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will inspect the vectors to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000apple</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>07115164</th>\n",
       "      <th>07airtag</th>\n",
       "      <th>08132321259</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>zac</th>\n",
       "      <th>zarak</th>\n",
       "      <th>zdnet</th>\n",
       "      <th>zdnets</th>\n",
       "      <th>zee</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000apple  03  05  07  07115164  07airtag  08132321259  09  ...  \\\n",
       "0   0    0         0   0   0   0         0         0            0   0  ...   \n",
       "1   0    0         0   0   0   0         0         0            0   0  ...   \n",
       "2   0    0         0   0   0   0         0         0            0   0  ...   \n",
       "3   0    0         0   0   0   0         0         0            0   0  ...   \n",
       "4   0    0         0   0   0   0         0         0            0   0  ...   \n",
       "\n",
       "   yup  zac  zarak  zdnet  zdnets  zee  zero  zip  zone  zoom  \n",
       "0    0    0      0      0       0    0     0    0     0     0  \n",
       "1    0    0      0      0       0    0     0    0     0     0  \n",
       "2    0    0      0      0       0    0     0    0     0     0  \n",
       "3    0    0      0      0       0    0     0    0     0     0  \n",
       "4    0    0      0      0       0    0     0    0     0     0  \n",
       "\n",
       "[5 rows x 3783 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>airtag</th>\n",
       "      <td>3038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>1559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hacked</th>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researcher</th>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracker</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airtags</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lost</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "airtag      3038\n",
       "apple       1559\n",
       "new          282\n",
       "find         276\n",
       "hacked       255\n",
       "researcher   244\n",
       "security     241\n",
       "tracker      227\n",
       "airtags      213\n",
       "lost         182\n",
       "already      161"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the most used words in the training set by sorting values descendingly\n",
    "count = pd.DataFrame(count_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the hashtag keyword airtag, the most common words used were \"apple\"(AirTag's company), \"new\", \"find\", \"hacked\", \"researcher\", \"security\", \"tracker\", \"airtags\", \"lost\", and \"already\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Training and testing the \"Negative Tweet Detector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). We'll first train and test a Naive Bayes model using the CountVectorizer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only 14% of class 1 and 86% of class 0. Accuracy is not the metric to use when working with an imbalanced dataset since we could have the accuracy paradox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "# Define scoring metrics\n",
    "scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "# Use Cross-validation to evaluate the model\n",
    "cv_results = cross_validate(nb_classifier, count_train, y_train, cv=5,scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time :  0.004010915756225586\n",
      "score_time :  0.0030123710632324217\n",
      "test_accuracy :  0.872341523848854\n",
      "test_precision :  0.5432328786936461\n",
      "test_recall :  0.5504232441088995\n",
      "test_f1 :  0.545932682299735\n"
     ]
    }
   ],
   "source": [
    "# Print the mean cross validation score for each metrics\n",
    "for key in cv_results:\n",
    "    print(key,': ', np.mean(cv_results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will **tune** the Naive Bayes classifier by trying out several alpha values to see which one yeilds the best performance.  (The default alpha value for Multinomial Naive Bayes classifier was 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "fit_time :  0.0034763336181640623\n",
      "score_time :  0.003389263153076172\n",
      "test_accuracy :  0.8873223567856791\n",
      "test_precision :  0.5976697820733202\n",
      "test_recall :  0.5952871196522536\n",
      "test_f1 :  0.5962756384752638\n",
      "\n",
      "Alpha:  0.1\n",
      "fit_time :  0.0015959739685058594\n",
      "score_time :  0.002991914749145508\n",
      "test_accuracy :  0.8603506629918574\n",
      "test_precision :  0.5005561849111642\n",
      "test_recall :  0.6959505833905284\n",
      "test_f1 :  0.5817408806724007\n",
      "\n",
      "Alpha:  0.2\n",
      "fit_time :  0.001596212387084961\n",
      "score_time :  0.0027923107147216795\n",
      "test_accuracy :  0.8546566537090735\n",
      "test_precision :  0.4864664449206434\n",
      "test_recall :  0.695973461450469\n",
      "test_f1 :  0.5722637412984226\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "fit_time :  0.0015958309173583984\n",
      "score_time :  0.0031913280487060546\n",
      "test_accuracy :  0.8531574033342609\n",
      "test_precision :  0.4827224771305293\n",
      "test_recall :  0.6767101349805535\n",
      "test_f1 :  0.5627636869386878\n",
      "\n",
      "Alpha:  0.4\n",
      "fit_time :  0.0013968944549560547\n",
      "score_time :  0.003191089630126953\n",
      "test_accuracy :  0.857654705581341\n",
      "test_precision :  0.49339973548106786\n",
      "test_recall :  0.6660260809883323\n",
      "test_f1 :  0.5663613372178633\n",
      "\n",
      "Alpha:  0.5\n",
      "fit_time :  0.0021944522857666017\n",
      "score_time :  0.0029918670654296873\n",
      "test_accuracy :  0.8594542549084739\n",
      "test_precision :  0.4968871208916853\n",
      "test_recall :  0.6446122168840083\n",
      "test_f1 :  0.560745955321447\n",
      "\n",
      "Alpha:  0.6000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abby\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "C:\\Users\\Abby\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "C:\\Users\\Abby\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "C:\\Users\\Abby\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "C:\\Users\\Abby\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time :  0.0017955780029296875\n",
      "score_time :  0.0027923583984375\n",
      "test_accuracy :  0.8645490129186904\n",
      "test_precision :  0.5117885789662385\n",
      "test_recall :  0.6231983527796843\n",
      "test_f1 :  0.5616363697037045\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "fit_time :  0.0023941993713378906\n",
      "score_time :  0.003190279006958008\n",
      "test_accuracy :  0.8657466177091095\n",
      "test_precision :  0.5170530541583174\n",
      "test_recall :  0.5932052161976664\n",
      "test_f1 :  0.552132001536439\n",
      "\n",
      "Alpha:  0.8\n",
      "fit_time :  0.001994419097900391\n",
      "score_time :  0.0033908367156982424\n",
      "test_accuracy :  0.8666461679339971\n",
      "test_precision :  0.5211368986361002\n",
      "test_recall :  0.5696636925188743\n",
      "test_f1 :  0.54391351030959\n",
      "\n",
      "Alpha:  0.9\n",
      "fit_time :  0.0017955303192138672\n",
      "score_time :  0.003789377212524414\n",
      "test_accuracy :  0.8687437718266615\n",
      "test_precision :  0.5293105274491254\n",
      "test_recall :  0.5589567604667124\n",
      "test_f1 :  0.5432705427504866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "# Define scoring metrics\n",
    "scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Use Cross-validation to evaluate the model\n",
    "    cv_results = cross_validate(nb_classifier, count_train, y_train, cv=5,scoring=scoring)\n",
    "    for key in cv_results:\n",
    "        print(key,': ', np.mean(cv_results[key]))    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the main purpose of our classifier is to detect as many negative tweets as possible, we will see the recall score as the most important metric as it measures the coverage of actual positive samples. According to the results above, I will pick 0.1 as the best alpha for my Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time :  0.061238861083984374\n",
      "score_time :  0.0035968780517578124\n",
      "test_accuracy :  0.9196891075420375\n",
      "test_precision :  0.8886842306426239\n",
      "test_recall :  0.48833218943033624\n",
      "test_f1 :  0.626209757683717\n"
     ]
    }
   ],
   "source": [
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "# Define scoring metrics\n",
    "scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "# Use Cross-validation to evaluate the model\n",
    "cv_results = cross_validate(logreg, count_train, y_train, cv=5,scoring=scoring)\n",
    "# Print the mean cross validation score for each metrics\n",
    "for key in cv_results:\n",
    "    print(key,': ', np.mean(cv_results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will tune the Logistic Regression classifier by trying out several C values to see which one yeilds the best performance. (The default C value for Logistic Regression classifier is 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.5\n",
      "fit_time :  0.03651537895202637\n",
      "score_time :  0.002987241744995117\n",
      "test_accuracy :  0.9145939006544632\n",
      "test_precision :  0.9236246949290428\n",
      "test_recall :  0.4261725005719515\n",
      "test_f1 :  0.5807533387945887\n",
      "\n",
      "C:  1.5\n",
      "fit_time :  0.04965529441833496\n",
      "score_time :  0.003197002410888672\n",
      "test_accuracy :  0.920588208889567\n",
      "test_precision :  0.888020313020313\n",
      "test_recall :  0.4969114619080302\n",
      "test_f1 :  0.6335627624663321\n",
      "\n",
      "C:  2.5\n",
      "fit_time :  0.05585112571716309\n",
      "score_time :  0.0027965545654296876\n",
      "test_accuracy :  0.9217853648026286\n",
      "test_precision :  0.8541069990461121\n",
      "test_recall :  0.5332875772134523\n",
      "test_f1 :  0.6539758715716903\n",
      "\n",
      "C:  3.5\n",
      "fit_time :  0.05845065116882324\n",
      "score_time :  0.0025945663452148437\n",
      "test_accuracy :  0.9211861135300612\n",
      "test_precision :  0.8435276638584988\n",
      "test_recall :  0.5375886524822695\n",
      "test_f1 :  0.6536863569858513\n",
      "\n",
      "C:  4.5\n",
      "fit_time :  0.06204352378845215\n",
      "score_time :  0.002791881561279297\n",
      "test_accuracy :  0.9220852148775911\n",
      "test_precision :  0.8436882209481478\n",
      "test_recall :  0.5461679249599634\n",
      "test_f1 :  0.6599309125668149\n",
      "\n",
      "C:  5.5\n",
      "fit_time :  0.06463503837585449\n",
      "score_time :  0.002991390228271484\n",
      "test_accuracy :  0.922384616075196\n",
      "test_precision :  0.8425068159686699\n",
      "test_recall :  0.55044612216884\n",
      "test_f1 :  0.6626336833648239\n",
      "\n",
      "C:  6.5\n",
      "fit_time :  0.0861778736114502\n",
      "score_time :  0.0029937744140625\n",
      "test_accuracy :  0.9229838673477634\n",
      "test_precision :  0.8409053858574861\n",
      "test_recall :  0.5568519789521849\n",
      "test_f1 :  0.6671677344281715\n",
      "\n",
      "C:  7.5\n",
      "fit_time :  0.07041807174682617\n",
      "score_time :  0.002795553207397461\n",
      "test_accuracy :  0.9232837174227256\n",
      "test_precision :  0.8415787865308868\n",
      "test_recall :  0.5590025165865934\n",
      "test_f1 :  0.6690614648933305\n",
      "\n",
      "C:  8.5\n",
      "fit_time :  0.07181053161621094\n",
      "score_time :  0.0029823780059814453\n",
      "test_accuracy :  0.9232837174227259\n",
      "test_precision :  0.8399710421405336\n",
      "test_recall :  0.561153054221002\n",
      "test_f1 :  0.669814401192965\n",
      "\n",
      "C:  9.5\n",
      "fit_time :  0.07101993560791016\n",
      "score_time :  0.003191566467285156\n",
      "test_accuracy :  0.9235831186203306\n",
      "test_precision :  0.840375082544574\n",
      "test_recall :  0.5632807137954702\n",
      "test_f1 :  0.6716282956243089\n",
      "\n",
      "C:  10.5\n",
      "fit_time :  0.07539930343627929\n",
      "score_time :  0.002995872497558594\n",
      "test_accuracy :  0.9235831186203306\n",
      "test_precision :  0.8381422276801402\n",
      "test_recall :  0.5654312514298787\n",
      "test_f1 :  0.672673340669354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of Cs\n",
    "cs = np.arange(0.5, 11, 1)\n",
    "# Define scoring metrics\n",
    "scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "# Iterate over the Cs and print the corresponding score\n",
    "for c in cs:\n",
    "    print('C: ', c)\n",
    "    # Instantiate the classifier: \n",
    "    logreg = LogisticRegression(C=c)\n",
    "    # Use Cross-validation to evaluate the model\n",
    "    cv_results = cross_validate(logreg, count_train, y_train, cv=5,scoring=scoring)\n",
    "    for key in cv_results:\n",
    "        print(key,': ', np.mean(cv_results[key]))    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results above, I will pick 10.5 as the best C for my Logistic Regression classifier since it gives the highest accuracy and recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'll compare the performances of MultinomialNB(alpha=0.1) and LogisticRegression(C=10.5). I'll use recall as my metric for final selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1 Performance of  the Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6959505833905284 0.07205608326270745\n"
     ]
    }
   ],
   "source": [
    "# Evaluate recall by cross-validation using training set\n",
    "nb_recall = cross_validate(MultinomialNB(alpha=0.1), count_train, y_train, \n",
    "                            scoring='recall', cv=5, n_jobs=-1)\n",
    "print(np.mean(nb_recall['test_score']), np.std(nb_recall['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1336   71]\n",
      " [  90  148]]\n",
      "The recall score is: 0.6218487394957983\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the Naive Bayes Classifier using the test set\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test,pred)\n",
    "print(cm)\n",
    "# extract true positives, false positive, false negative, and false positive\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,pred).ravel()\n",
    "print(\"The recall score is:\", tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 Performance of  the Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5654312514298787 0.06306498130540593\n"
     ]
    }
   ],
   "source": [
    "# Evaluate recall by cross-validation using training set\n",
    "logreg_recall = cross_validate(LogisticRegression(C=10.5), count_train, y_train, \n",
    "                            scoring='recall', cv=5, n_jobs=-1)\n",
    "print(np.mean(logreg_recall['test_score']), np.std(logreg_recall['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      " [[1387   20]\n",
      " [  90  148]]\n",
      "The recall score is: 0.6218487394957983\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the Logistic Regression Classifier using the test set\n",
    "logreg = LogisticRegression(C=10.5)\n",
    "logreg.fit(count_train,y_train)\n",
    "y_pred = logreg.predict(count_test)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print('confusion matrix:\\n',cm)\n",
    "# extract true positives, false positive, false negative, and false positive\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
    "print(\"The recall score is:\", tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 Conslusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifier has a higher cross-validation recall score by 0.13. Verifying performance on the test set shows that the recall score for both models are the same. I'll pick the Multinomial Naive Bayes classifier as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model object\n",
    "best_model = MultinomialNB(alpha=0.1)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare your data for fitting models, I've performed NLP precrocessing steps to the tweet text. The steps included tokenization to split sentences into tokens, removing stop words, and lemmatization to convert words to their meaningful base forms. I then use CountVectorizer to vectorizing the dataset, which represented text as numerical data for modeling.   \n",
    "Only the preprocessed tweet text column was used to predict sentiment classes. To build a negative tweet dector, all tweets with 'postive' and 'neutral' sentiment labels were relabeled as 0, and tweets with 'negative' sentiment label were relabeled as 1.  \n",
    "We then have a binary classification problem. Here we have tried two classificaiton models: Multinomial Naive Bayes classifier and Logistic Regression Classifier.  \n",
    "Evaluating the performance of a model by training and testing on the same dataset can lead to overfitting. To prevent that, Cross-Validation technique is used where under the k-fold CV approach, the training set is split into k smaller sets, where a model is trained using k-1 of the folds as training data and the model is validated on the remaining part.  \n",
    "I've first tried using models with their default parameters to classify sentiments, and I've evaluated their performances in terms of accuracy, precision, recall, and f1 using cross-validation.  \n",
    "Next, I did hyperparameter tuning for both models seperately. With the result of the optimized hyperparameters, I have evaluated each model using recall score for both the training and test data using cross validation. I've picked the Multinomial Naive Bayes classifier with alpha equal to 0.1 as the final model since it gave a higher cross-validation recall score than the Logistic Regression Classifier by 0.13, and the performances on the test set were the same for both models in terms of recall score.\n",
    "Therefor, if we use our chosen model for negative tweets dectection, it's expected to dectect around 69% of negative tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
